<template>
  <div class="meeting">
    <div class="meeting-header">
      <h1>ä¼šè®®ï¼š{{ meetingName }}</h1>
      <div class="meeting-controls">
        <el-button type="success" @click="toggleRecording" :disabled="isRecordingDisabled">
          <span v-if="isRecording" class="recording-indicator">
            <span class="microphone-icon" :class="{ 'recording': isRecording, 'voice-detected': isVoiceDetected }">ğŸ¤</span>
          </span>
          {{ isRecording ? 'åœæ­¢å½•éŸ³' : 'å¼€å§‹å½•éŸ³' }}
        </el-button>
        <el-button type="danger" @click="endMeeting">ç»“æŸä¼šè®®</el-button>
      </div>
    </div>
    
    <div class="meeting-content">
      <div class="transcription-area">
        <h2>å®æ—¶è½¬å†™</h2>
        <div class="transcription-container" ref="transcriptionContainer">
          <el-skeleton :rows="3" animated v-if="loading" />
          <div v-else-if="transcriptions.length === 0" class="empty-transcription">
            <p>æš‚æ— è½¬å†™è®°å½•ï¼Œç‚¹å‡»å¼€å§‹å½•éŸ³å¼€å§‹ä¼šè®®</p>
          </div>
          <div v-for="speech in transcriptions" :key="speech.id" class="speech-item">
            <span :style="{ color: speech.color }" class="speaker-name">{{ speech.speaker }}ï¼š</span>
            <span class="speech-content">{{ speech.content }}</span>
            <el-button type="primary" size="small" @click="generateAIAnalysis(speech)">
              åˆ†æ
            </el-button>
          </div>
        </div>
      </div>
      
      <div class="ai-analysis-area">
        <h2>AIåˆ†æ</h2>
        <div v-if="!showAIResult" class="ai-prompt">
          <p>ç‚¹å‡»ä¸Šæ–¹å‘è¨€æ—è¾¹çš„"åˆ†æ"æŒ‰é’®æŸ¥çœ‹AIåˆ†æç»“æœ</p>
        </div>
        <el-dialog
          v-model="showAIResult"
          title="AIåˆ†æç»“æœ"
          width="800px"
        >
          <div v-if="selectedSpeech" class="selected-speech">
            <h3>{{ selectedSpeech.speaker }}çš„å‘è¨€ï¼š</h3>
            <p>{{ selectedSpeech.content }}</p>
          </div>
          
          <div class="ai-settings">
            <div class="ai-model-select">
              <span>AIæ¨¡å‹ï¼š</span>
              <el-select v-model="selectedModel" placeholder="é€‰æ‹©AIæ¨¡å‹">
                <el-option
                  v-for="model in aiModels"
                  :key="model.name"
                  :label="model.display_name"
                  :value="model.name"
                />
              </el-select>
              <el-button type="primary" size="small" @click="generateAIAnalysis(selectedSpeech)" :disabled="!selectedSpeech">
                é‡æ–°åˆ†æ
              </el-button>
            </div>
          </div>
          
          <div v-if="aiResult" class="ai-result">
            <el-tabs v-model="analysisType" type="card">
              <el-tab-pane label="æ ¸å¿ƒè¦ç‚¹" name="coreAnalysis">
                <div class="analysis-content">
                  <h3>æ ¸å¿ƒè¦ç‚¹åˆ†æï¼š</h3>
                  <p>{{ aiResult.coreAnalysis }}</p>
                </div>
              </el-tab-pane>
              <el-tab-pane label="ç®€è¦å›ç­”" name="briefAnswer">
                <div class="analysis-content">
                  <h3>ç®€è¦å›ç­”ï¼š</h3>
                  <p>{{ aiResult.briefAnswer }}</p>
                </div>
              </el-tab-pane>
              <el-tab-pane label="æ·±åº¦åˆ†æ" name="deepAnswer">
                <div class="analysis-content">
                  <h3>æ·±åº¦åˆ†æï¼š</h3>
                  <pre>{{ aiResult.deepAnswer }}</pre>
                </div>
              </el-tab-pane>
            </el-tabs>
          </div>
          <div v-else class="ai-loading">
            <el-skeleton :rows="6" animated />
          </div>
          <template #footer>
            <el-button @click="showAIResult = false">å…³é—­</el-button>
          </template>
        </el-dialog>
      </div>
    </div>
  </div>
</template>

<script setup lang="ts">
import { ref, onMounted, onBeforeUnmount } from 'vue'
import { useRouter } from 'vue-router'
import { useMeetingStore } from '../stores/meeting'
import { useTranscriptionStore } from '../stores/transcription'
import { meetingApi } from '../api/meeting'
import { aiApi } from '../api/ai'
import type { AIModel } from '../api/ai'
import type { Speech, AIAnalysis } from '../stores/transcription'
import { ElMessage } from 'element-plus'
import { AudioCaptureService } from '../services/audio-capture'
import { io, Socket } from 'socket.io-client'

const router = useRouter()
const meetingStore = useMeetingStore()
const transcriptionStore = useTranscriptionStore()

const props = defineProps<{
  id: string
}>()

const meetingName = ref('åŠ è½½ä¸­...')
const isRecording = ref(false)
const isRecordingDisabled = ref(false)
const loading = ref(false)
const transcriptions = ref<Speech[]>([])
const showAIResult = ref(false)
const selectedSpeech = ref<Speech | null>(null)
const aiResult = ref<AIAnalysis | null>(null)
const socket = ref<Socket | null>(null)
const isConnected = ref(false)
const transcriptionContainer = ref<HTMLElement | null>(null)
const isVoiceDetected = ref(false)
const voiceDetectionTimeout = ref<number | null>(null)
// ç”¨äºè·Ÿè¸ªç”¨æˆ·æ˜¯å¦æ‰‹åŠ¨æ»šåŠ¨
const isUserScrolling = ref(false)

// å‘è¨€è€…é¢œè‰²æ± 
const speakerColors = ['#1890ff', '#52c41a', '#faad14', '#f5222d', '#722ed1', '#13c2c2', '#eb2f96', '#fa8c16']
const speakerColorMap = new Map<string, string>()

// è·å–ä¼šè®®è¯¦æƒ…
const fetchMeetingDetail = async () => {
  loading.value = true
  try {
    const meeting = await meetingApi.getMeetingDetail(props.id)
    meetingName.value = meeting.name
    meetingStore.setCurrentMeeting(meeting)
  } catch (error: any) {
    ElMessage.error(error.message || 'è·å–ä¼šè®®è¯¦æƒ…å¤±è´¥')
    console.error('è·å–ä¼šè®®è¯¦æƒ…å¤±è´¥:', error)
  } finally {
    loading.value = false
  }
}

// åˆå§‹åŒ–Socket.ioè¿æ¥
const initWebSocket = () => {
  try {
    // ä½¿ç”¨Socket.ioè¿æ¥åˆ°åç«¯æœåŠ¡å™¨ï¼Œé€šè¿‡Viteä»£ç†
    socket.value = io('http://localhost:5102', {
      transports: ['websocket'],
      autoConnect: true
    })
    
    socket.value.on('connect', () => {
      console.log('Socket.ioè¿æ¥å·²å»ºç«‹')
      isConnected.value = true
      // åŠ å…¥ä¼šè¯
      socket.value?.emit('join_session', { session_id: props.id })
    })
    
    socket.value.on('transcription', (result: any) => {
      console.log('Socket.ioæ¶ˆæ¯:', result)
      
      // ç›´æ¥åœ¨æ§åˆ¶å°æ˜¾ç¤ºæ¥æ”¶åˆ°çš„è½¬å†™ç»“æœï¼Œä¾¿äºè°ƒè¯•
      console.log('æ¥æ”¶åˆ°è½¬å†™ç»“æœ:', result.text)
      
      // ä¸ºå‘è¨€è€…åˆ†é…é¢œè‰²
      if (!speakerColorMap.has(result.speaker)) {
        const colorIndex = speakerColorMap.size % speakerColors.length
        speakerColorMap.set(result.speaker, speakerColors[colorIndex] || '#1890ff')
      }
      
      // æ›´æ–°è½¬å†™è®°å½•
      const transcription = {
        id: Date.now().toString(),
        speaker: result.speaker,
        content: result.text,
        startTime: new Date().toISOString(),
        endTime: new Date().toISOString(),
        confidence: result.confidence,
        color: speakerColorMap.get(result.speaker) || '#1890ff'
      }
      
      console.log('å°†è½¬å†™ç»“æœæ·»åŠ åˆ°æ•°ç»„:', transcription)
      transcriptions.value.push(transcription)
      transcriptionStore.addTranscription(transcription)
      console.log('å½“å‰è½¬å†™è®°å½•æ•°é‡:', transcriptions.value.length)
      
      // è‡ªåŠ¨æ»šåŠ¨åˆ°åº•éƒ¨
      scrollToBottom()
    })
    
    socket.value.on('disconnect', () => {
      console.log('Socket.ioè¿æ¥å·²å…³é—­')
      isConnected.value = false
    })
    
    socket.value.on('connect_error', (error: any) => {
      console.error('Socket.ioè¿æ¥é”™è¯¯:', error)
      isConnected.value = false
    })
  } catch (error) {
    console.error('åˆå§‹åŒ–Socket.ioè¿æ¥å¤±è´¥:', error)
    isConnected.value = false
  }
}

// éŸ³é¢‘æ•è·æœåŠ¡å®ä¾‹
const audioCaptureService = ref<AudioCaptureService | null>(null)

// å¼€å§‹å½•éŸ³
const startRecording = async () => {
  if (!socket.value || !isConnected.value) {
    ElMessage.error('Socket.ioè¿æ¥æœªå»ºç«‹ï¼Œæ— æ³•å¼€å§‹å½•éŸ³')
    return
  }
  
  try {
    // å‘é€å¼€å§‹å½•éŸ³å‘½ä»¤
    console.log('å‘é€start_recordingäº‹ä»¶:', { session_id: props.id })
    socket.value.emit('start_recording', { session_id: props.id })
    
    // åˆå§‹åŒ–éŸ³é¢‘æ•è·æœåŠ¡
    audioCaptureService.value = new AudioCaptureService(socket.value)
    
    // å¼€å§‹éŸ³é¢‘æ•è·ï¼Œæ·»åŠ å›è°ƒç›‘å¬éŸ³é¢‘æ•°æ®å’ŒéŸ³é‡
    await audioCaptureService.value.startCapture((audioData, volume) => {
      console.log('éŸ³é¢‘æ•°æ®æ•è·æˆåŠŸ:', audioData.length, 'å­—èŠ‚', 'éŸ³é‡:', volume)
      
      // è¯­éŸ³æ£€æµ‹ï¼šæ ¹æ®éŸ³é‡å¤§å°åˆ¤æ–­æ˜¯å¦æœ‰è¯­éŸ³æ´»åŠ¨
      // éŸ³é‡é˜ˆå€¼ï¼Œè¶…è¿‡è¿™ä¸ªå€¼è¡¨ç¤ºæ£€æµ‹åˆ°è¯­éŸ³
      const volumeThreshold = 0.01
      if (volume > volumeThreshold) {
        isVoiceDetected.value = true
        
        // æ¸…é™¤ä¹‹å‰çš„è¶…æ—¶ï¼Œè®¾ç½®æ–°çš„è¶…æ—¶
        if (voiceDetectionTimeout.value) {
          clearTimeout(voiceDetectionTimeout.value)
        }
        // 500msåæ¢å¤æ­£å¸¸çŠ¶æ€ï¼Œä½¿æŠ–åŠ¨æ•ˆæœæ›´çµæ•
        voiceDetectionTimeout.value = window.setTimeout(() => {
          isVoiceDetected.value = false
        }, 500)
      }
    })
    
    console.log('å½•éŸ³å·²å¼€å§‹ï¼Œç­‰å¾…è½¬å†™ç»“æœ...')
    
    isRecording.value = true
    meetingStore.startRecording()
    
    ElMessage.success('å½•éŸ³å·²å¼€å§‹')
    console.log('å¼€å§‹å½•éŸ³å®Œæˆ')
  } catch (error: any) {
    ElMessage.error('è·å–éº¦å…‹é£æƒé™å¤±è´¥æˆ–å½•éŸ³åˆå§‹åŒ–å¤±è´¥')
    console.error('å¼€å§‹å½•éŸ³å¤±è´¥:', error)
    isRecording.value = false
  }
}

// åœæ­¢å½•éŸ³
const stopRecording = () => {
  if (audioCaptureService.value) {
    audioCaptureService.value.stopCapture()
    audioCaptureService.value = null
  }
  
  // å‘é€åœæ­¢å½•éŸ³å‘½ä»¤
  socket.value?.emit('stop_recording', { session_id: props.id })
  
  isRecording.value = false
  meetingStore.stopRecording()
  
  ElMessage.success('å½•éŸ³å·²åœæ­¢')
  console.log('åœæ­¢å½•éŸ³')
}

// åˆ‡æ¢å½•éŸ³çŠ¶æ€
const toggleRecording = () => {
  if (isRecording.value) {
    stopRecording()
  } else {
    startRecording()
  }
}

// ç»“æŸä¼šè®®
const endMeeting = async () => {
  try {
    await meetingApi.endMeeting(props.id)
    ElMessage.success('ä¼šè®®å·²ç»“æŸ')
    meetingStore.endMeeting()
    router.push('/')
  } catch (error: any) {
    ElMessage.error(error.message || 'ç»“æŸä¼šè®®å¤±è´¥')
    console.error('ç»“æŸä¼šè®®å¤±è´¥:', error)
  }
}

// AIæ¨¡å‹ç›¸å…³çŠ¶æ€
const aiModels = ref<AIModel[]>([])
const selectedModel = ref<string>('qianwen')
const analysisType = ref<string>('coreAnalysis') // coreAnalysis, briefAnswer, deepAnswer

// è·å–æ”¯æŒçš„AIæ¨¡å‹åˆ—è¡¨
const fetchAIModels = async () => {
  try {
    const models = await aiApi.getAIModels()
    aiModels.value = models
    // è®¾ç½®é»˜è®¤æ¨¡å‹
    const defaultModel = models.find((model) => model.is_default)
    if (defaultModel) {
      selectedModel.value = defaultModel.name
    }
  } catch (error: any) {
    ElMessage.error(error.message || 'è·å–AIæ¨¡å‹åˆ—è¡¨å¤±è´¥')
    console.error('è·å–AIæ¨¡å‹åˆ—è¡¨å¤±è´¥:', error)
    // ä½¿ç”¨é»˜è®¤æ¨¡å‹åˆ—è¡¨
    aiModels.value = [
      { name: 'qianwen', display_name: 'å­—èŠ‚è·³åŠ¨åƒé—®', description: 'å­—èŠ‚è·³åŠ¨å¼€å‘çš„å¤§è¯­è¨€æ¨¡å‹', is_default: true, cost_per_1k_tokens: 0.15 },
      { name: 'doubao', display_name: 'è±†åŒ…', description: 'å­—èŠ‚è·³åŠ¨å¼€å‘çš„AIå¯¹è¯åŠ©æ‰‹', is_default: false, cost_per_1k_tokens: 0.2 },
      { name: 'glm-4', display_name: 'æ™ºè°±GLM-4', description: 'æ™ºè°±AIå¼€å‘çš„å¤§è¯­è¨€æ¨¡å‹', is_default: false, cost_per_1k_tokens: 0.2 },
      { name: 'minimax', display_name: 'MINIMAX', description: 'MINIMAXå¼€å‘çš„å¤§è¯­è¨€æ¨¡å‹', is_default: false, cost_per_1k_tokens: 0.3 },
      { name: 'kimi', display_name: 'KIMI', description: 'æœˆä¹‹æš—é¢å¼€å‘çš„å¤§è¯­è¨€æ¨¡å‹', is_default: false, cost_per_1k_tokens: 0.4 },
      { name: 'dc', display_name: 'æ·±åº¦æ±‚ç´¢', description: 'æ·±åº¦æ±‚ç´¢å¼€å‘çš„å¤§è¯­è¨€æ¨¡å‹', is_default: false, cost_per_1k_tokens: 0.5 }
    ]
  }
}

// ç”ŸæˆAIåˆ†æ
const generateAIAnalysis = async (speech: Speech | null) => {
  if (!speech) return
  
  selectedSpeech.value = speech
  showAIResult.value = true
  aiResult.value = null
  analysisType.value = 'coreAnalysis'
  
  try {
    const analysis = await aiApi.analyzeSpeech(speech.id, {
      model_name: selectedModel.value,
      prompt: 'è¯·åˆ†æä»¥ä¸‹å‘è¨€å†…å®¹çš„æ ¸å¿ƒè¦ç‚¹ã€ç®€è¦å›ç­”å’Œæ·±åº¦åˆ†æï¼š'
    })
    aiResult.value = analysis
    transcriptionStore.setAIResult(analysis)
  } catch (error: any) {
    ElMessage.error(error.message || 'ç”ŸæˆAIåˆ†æå¤±è´¥')
    console.error('ç”ŸæˆAIåˆ†æå¤±è´¥:', error)
  }
}

// è‡ªåŠ¨æ»šåŠ¨åˆ°åº•éƒ¨
const scrollToBottom = () => {
  if (!transcriptionContainer.value) return
  
  // å¦‚æœç”¨æˆ·æ²¡æœ‰æ‰‹åŠ¨æ»šåŠ¨ï¼Œè‡ªåŠ¨æ»šåŠ¨åˆ°åº•éƒ¨
  if (!isUserScrolling.value) {
    setTimeout(() => {
      if (transcriptionContainer.value) {
        transcriptionContainer.value.scrollTop = transcriptionContainer.value.scrollHeight
      }
    }, 100)
  }
}

// ç›‘å¬æ»šåŠ¨äº‹ä»¶ï¼Œåˆ¤æ–­ç”¨æˆ·æ˜¯å¦æ‰‹åŠ¨æ»šåŠ¨
const handleScroll = () => {
  if (!transcriptionContainer.value) return
  
  const currentScrollTop = transcriptionContainer.value.scrollTop
  const scrollHeight = transcriptionContainer.value.scrollHeight
  const clientHeight = transcriptionContainer.value.clientHeight
  
  // è®¡ç®—æ»šåŠ¨åˆ°åº•éƒ¨çš„é˜ˆå€¼ï¼ˆ10pxå†…ï¼‰
  const isAtBottom = scrollHeight - currentScrollTop - clientHeight < 10
  
  if (isAtBottom) {
    isUserScrolling.value = false
  } else {
    isUserScrolling.value = true
  }
}

// é¡µé¢åŠ è½½æ—¶è·å–ä¼šè®®è¯¦æƒ…å’Œåˆå§‹åŒ–WebSocketè¿æ¥
onMounted(() => {
  fetchMeetingDetail()
  initWebSocket()
  fetchAIModels()
  
  // æ·»åŠ æ»šåŠ¨äº‹ä»¶ç›‘å¬
  if (transcriptionContainer.value) {
    transcriptionContainer.value.addEventListener('scroll', handleScroll)
  }
})

// é¡µé¢å¸è½½å‰å…³é—­WebSocketè¿æ¥
onBeforeUnmount(() => {
  if (socket.value) {
    socket.value.close()
  }
  
  // åœæ­¢å½•éŸ³
  if (isRecording.value) {
    stopRecording()
  }
  
  // ç§»é™¤æ»šåŠ¨äº‹ä»¶ç›‘å¬
  if (transcriptionContainer.value) {
    transcriptionContainer.value.removeEventListener('scroll', handleScroll)
  }
})
</script>

<style scoped>
.meeting {
  padding: 20px;
}

.meeting-header {
  display: flex;
  justify-content: space-between;
  align-items: center;
  margin-bottom: 20px;
}

.meeting-controls {
  display: flex;
  gap: 10px;
}

.meeting-content {
  display: grid;
  grid-template-columns: 1fr 1fr;
  gap: 20px;
  height: calc(100vh - 150px);
}

.transcription-area,
.ai-analysis-area {
  background: #f5f5f5;
  border-radius: 8px;
  padding: 15px;
  overflow: hidden;
  display: flex;
  flex-direction: column;
}

.transcription-container {
  flex: 1;
  overflow-y: auto;
  margin-top: 10px;
  padding: 10px;
  background: white;
  border-radius: 4px;
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
}

.speech-item {
  margin-bottom: 10px;
  padding: 8px;
  border-radius: 4px;
  background: #f9f9f9;
  display: flex;
  align-items: flex-start;
  gap: 10px;
}

.speaker-name {
  font-weight: bold;
  min-width: 80px;
}

.speech-content {
  flex: 1;
  word-wrap: break-word;
}

.empty-transcription {
  text-align: center;
  color: #999;
  padding: 50px 0;
}

.ai-prompt {
  text-align: center;
  color: #999;
  padding: 50px 0;
  background: white;
  border-radius: 4px;
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
  margin-top: 10px;
}

.selected-speech {
  margin-bottom: 20px;
  padding: 10px;
  background: #f0f9ff;
  border-radius: 4px;
  border-left: 4px solid #1890ff;
}

.ai-settings {
  margin-bottom: 20px;
  padding: 10px;
  background: #f9f9f9;
  border-radius: 4px;
}

.ai-model-select {
  display: flex;
  align-items: center;
  gap: 10px;
}

.ai-result {
  margin-top: 10px;
}

.analysis-content {
  margin-top: 10px;
  padding: 10px;
  background: white;
  border-radius: 4px;
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
}

.ai-loading {
  margin-top: 10px;
}

/* éº¦å…‹é£å›¾æ ‡æ ·å¼ */
.recording-indicator {
  margin-right: 8px;
  display: inline-flex;
  align-items: center;
}

.microphone-icon {
  font-size: 16px;
  transition: all 0.3s ease;
}

/* å½•éŸ³çŠ¶æ€æ ·å¼ */
.microphone-icon.recording {
  color: #f56c6c;
  animation: pulse 1s infinite;
}

/* è¯­éŸ³æ£€æµ‹æ ·å¼ */
.microphone-icon.voice-detected {
  color: #67c23a;
  animation: shake 0.5s ease-in-out;
}

/* è„‰å†²åŠ¨ç”» */
@keyframes pulse {
  0% {
    transform: scale(1);
    opacity: 1;
  }
  50% {
    transform: scale(1.2);
    opacity: 0.8;
  }
  100% {
    transform: scale(1);
    opacity: 1;
  }
}

/* æŠ–åŠ¨åŠ¨ç”» */
@keyframes shake {
  0%, 100% {
    transform: translateX(0);
  }
  20%, 60% {
    transform: translateX(-5px);
  }
  40%, 80% {
    transform: translateX(5px);
  }
}
</style>